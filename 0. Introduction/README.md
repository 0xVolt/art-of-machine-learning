# 0. Introduction
This section of the repository will contain an introduction to machine learning, the way I am documenting my study of it and a plan on covering concepts. This is going to give me an opportunity to revisit all of the math behind some of machine learning's most fundamental constructs.

Everything learnt on this journey to revisit machine learning concepts and study further, is from [this YouTube playlist](https://www.youtube.com/playlist?list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v) by [sentdex](https://www.youtube.com/@sentdex).

## What is machine learning?

### At it's core
Arthur Samuel talked about machine learning being the field of study where we give machines the ability to learn without explicitly programming them to do so. A great way to think about it is to imbue knowledge to a machine without programming that knowledge in the first place and sort of allowing the machine (model, more precisely) to figure it out through a minimisation of loss and/or iterative approach to improve accuracy.

### What people think it is
A common misconception is that a large portion of machine learning or rather, it's application is hardcoded when that couldn't be further from the truth. Most of machine learning isn't hardcoded and it's about finding the best way for the machine (model) to learn with as little effort from the programmer.

### Where machine learning is after the 50 years
Through the last half-century, there's been a battle for the title of best performing model - with the case of character recognition in handwriting being the convention to test a model's efficacy - between neural networks and the support vector machine. The SVM was co-developed by Vladimir Vapnik in the 60s. However, once he was recruited by Bell Labs and flown out of the Soviet Union, it was only in the 90s did the machine learning community see what the SVM was capable of. In fact, it showed even higher accuracy than neural networks (at the time). It wasn't until extremely recently did Google make the push to show how capable neural networks could be, with deep learning.

### No limits
Thinking about computers in the 50s, where transistors were only then being put onto printed circuits, to those in the 90s where researchers could barely get an SVM that could run at the required scale. We've covered significant lengths to be where we are right now where anyone could just rent GPUs to engage in deep learning on gigabytes or terabytes of data. This is the best time to be alive (from a machine learning standpoint) and it seems like there are no limits to which this field of study will progress in our time.